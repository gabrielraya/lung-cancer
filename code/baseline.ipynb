{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'data_to_csv' from 'preprocessing' (/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/code/preprocessing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e44ad1737488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradcam_wsi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradcam_on_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_to_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_csv_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_wsi_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_file_exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'data_to_csv' from 'preprocessing' (/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/code/preprocessing.py)"
     ]
    }
   ],
   "source": [
    "# Import NIC to python path\n",
    "import sys\n",
    "\n",
    "nic_dir = '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/code/neural-image-compression-private'\n",
    "sys.path.append(nic_dir + '/source')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import os, shutil\n",
    "from os.path import join, dirname, exists\n",
    "import keras\n",
    "from gradcam_wsi import gradcam_on_dataset\n",
    "from preprocessing import data_to_csv, create_csv, generate_csv_files\n",
    "from model_training import train_wsi_classifier, eval_model, compute_metrics\n",
    "from utils import check_file_exists\n",
    "\n",
    "\n",
    "def train_model(featurized_dir, csv_path, fold_n, output_dir, cache_dir, batch_size=16, epochs=32,\n",
    "                images_dir=None, vectorized_dir=None, lr=1e-2, patience=4, delete_folder=False,\n",
    "                occlusion_augmentation=False, elastic_augmentation=False, shuffle_augmentation=None):\n",
    "    \"\"\"\n",
    "    Trains a CNN using compressed whole-slide images.\n",
    "\n",
    "    :param featurized_dir: folder containing the compressed (featurized) images.\n",
    "    :param csv_path: list of slides with labels.\n",
    "    :param fold_n: fold determining which data partitions to use for training, validation and testing.\n",
    "    :param output_dir: destination folder to store results.\n",
    "    :param cache_dir: folder to store compressed images temporarily for fast access.\n",
    "    :param batch_size: number of samples to train with in one-go.\n",
    "    :return: nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Delete folder and subfolders if exists\n",
    "    if delete_folder:\n",
    "        if exists(result_dir):  shutil.rmtree(result_dir)\n",
    "\n",
    "    # Train CNN\n",
    "    train_wsi_classifier(\n",
    "        data_dir=featurized_dir,\n",
    "        csv_path=csv_path,\n",
    "        partitions=None,\n",
    "        crop_size=400,\n",
    "        output_dir=output_dir,\n",
    "        output_units=2,\n",
    "        cache_dir=cache_dir,\n",
    "        n_epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        code_size=128,\n",
    "        workers=1,\n",
    "        train_step_multiplier=1,\n",
    "        val_step_multiplier=0.5,\n",
    "        keep_data_training=1,\n",
    "        keep_data_validation=1,\n",
    "        patience=patience,\n",
    "        occlusion_augmentation=occlusion_augmentation,\n",
    "        elastic_augmentation=elastic_augmentation,\n",
    "        shuffle_augmentation=shuffle_augmentation\n",
    "    )\n",
    "\n",
    "    # Evaluate CNN\n",
    "\n",
    "    # Get compressed wsi directories with csv test file\n",
    "    data_config = featurized_dir\n",
    "    data_config['csv_path'] = csv_path['csv_test']\n",
    "\n",
    "    eval_model(\n",
    "        model_path=join(output_dir, 'checkpoint.h5'),\n",
    "        data_config=data_config,\n",
    "        crop_size=400,\n",
    "        output_path=join(output_dir, 'eval', 'preds.csv'),\n",
    "        cache_dir=None,\n",
    "        batch_size=batch_size,\n",
    "        keep_data=1\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    try:\n",
    "        compute_metrics(\n",
    "            input_path=join(output_dir, 'eval', 'preds.csv'),\n",
    "            output_dir=join(output_dir, 'eval')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to compute metrics. Exception: {e}'.format(e=e), flush=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating compressed wsi csv file ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6d07a5db7bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Create csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating compressed wsi csv file ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mcreate_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturized_luad_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturized_lusc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path_compressed_wsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating split train/validation/test csv files with no augmentations ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Creates csv from original data\n",
    "\n",
    "# project and data directories\n",
    "root_dir = r'/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction'\n",
    "data_dir = r'/mnt/netcache/pathology/archives/lung'\n",
    "\n",
    "# wsi directories\n",
    "dir_luad_wsi = os.path.join(data_dir, 'TCGA_LUAD', 'wsi_diagnostic_tif')\n",
    "dir_lusc_wsi = os.path.join(data_dir, 'TCGA_LUSC', 'wsi_diagnostic_tif')\n",
    "dir_luad_wsi_mask = os.path.join(data_dir, 'TCGA_LUAD', 'tissue_masks_diagnostic')\n",
    "dir_lusc_wsi_mask = os.path.join(data_dir, 'TCGA_LUSC', 'tissue_masks_diagnostic')\n",
    "\n",
    "# compressed image directories\n",
    "vectorized_luad_dir = join(root_dir, 'results', 'tcga_luad', 'vectorized')\n",
    "vectorized_lusc_dir = join(root_dir, 'results', 'tcga_lusc', 'vectorized')\n",
    "featurized_luad_dir = join(root_dir, 'results', 'tcga_luad', 'featurized', 'no_augmentations')\n",
    "featurized_lusc_dir = join(root_dir, 'results', 'tcga_lusc', 'featurized', 'no_augmentations')\n",
    "\n",
    "# results directory\n",
    "result_dir = join(root_dir, 'results', 'model')  # store the results from trained model\n",
    "gradcam_dir = join(result_dir, 'gradcam')  # store gradcam results\n",
    "\n",
    "# Set paths\n",
    "model_path = './neural-image-compression-private/models/encoders_patches_pathology/encoder_bigan.h5'\n",
    "csv_train = os.path.join(root_dir, 'data', 'train_slide_list_tcga.csv')\n",
    "csv_val = os.path.join(root_dir, 'data', 'validation_slide_list_tcga.csv')\n",
    "csv_test = os.path.join(root_dir, 'data', 'test_slide_list_tcga.csv')\n",
    "\n",
    "# csv paths\n",
    "csv_path_wsi = os.path.join(root_dir, 'data', 'slide_original_list_tcga.csv')\n",
    "csv_path_compressed_wsi = os.path.join(root_dir, 'data', 'slide_compressed_list_tcga.csv')\n",
    "\n",
    "cache_dir = None  # used to store local copies of files during I/O operations (useful in cluster\n",
    "\n",
    "\n",
    "# Train CNN\n",
    "\n",
    "# selected_fold = 0\n",
    "\n",
    "featurized_dir = {'data_dir_luad': featurized_luad_dir, 'data_dir_lusc': featurized_lusc_dir}\n",
    "csv_path = {'csv_train': csv_train, 'csv_val': csv_val, 'csv_test': csv_test}\n",
    "\n",
    "\n",
    "# Create csv files \n",
    "print('Creating compressed wsi csv file ...')\n",
    "create_csv(featurized_luad_dir, featurized_lusc_dir, csv_path_compressed_wsi)\n",
    "\n",
    "print('Creating split train/validation/test csv files with no augmentations ...')\n",
    "generate_csv_files(csv_path_compressed_wsi, csv_train, csv_val, csv_test, test_size=0.2, validation_size = 0.3)\n",
    "\n",
    "# read files to check shapes\n",
    "df = pd.read_csv(csv_train);  df2 = pd.read_csv(csv_val);   df3 = pd.read_csv(csv_test)\n",
    "print(f'Files were read with shapes: Training: {df.shape}, Validation {df2.shape}, Testing {df3.shape}')\n",
    "print(f'Total files: Files were read with shapes: {df.shape[0]+df2.shape[0]+df3.shape[0]}')\n",
    "\n",
    "train_model(\n",
    "    featurized_dir=featurized_dir,\n",
    "    csv_path=csv_path,\n",
    "    fold_n=0,\n",
    "    output_dir=result_dir,\n",
    "    cache_dir=None,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    delete_folder=False,\n",
    "    occlusion_augmentation=False,\n",
    "    lr=1e-2,\n",
    "    patience=4,\n",
    "    elastic_augmentation=False,\n",
    "    images_dir=None,  # required for GradCAM\n",
    "    vectorized_dir=None,  # required for GradCAM\n",
    "    shuffle_augmentation=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
