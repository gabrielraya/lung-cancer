{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data to local instance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NIC to python path\n",
    "import sys\n",
    "import os \n",
    "\n",
    "nic_dir = '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/code/neural-image-compression-private'\n",
    "sys.path.append(nic_dir + '/source')\n",
    "\n",
    "# Copy data\n",
    "print('Copying data to local instance')\n",
    "os.system('mkdir /home/user/featurized_tcga_luad/')\n",
    "os.system('mkdir /home/user/featurized_tcga_lusc/')\n",
    "os.system('cp /mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_luad/featurized/no_augmentations/* /home/user/featurized_tcga_luad')\n",
    "os.system('cp /mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_lusc/featurized/no_augmentations/* /home/user/featurized_tcga_lusc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import os, shutil\n",
    "from os.path import join, dirname, exists\n",
    "import keras\n",
    "from gradcam_wsi import gradcam_on_dataset\n",
    "from preprocessing import data_to_csv, create_csv, generate_csv_files\n",
    "from model_training import train_wsi_classifier, eval_model, compute_metrics\n",
    "from utils import check_file_exists\n",
    "\n",
    "\n",
    "def train_model(featurized_dir, csv_path, fold_n, output_dir, cache_dir, batch_size=16, epochs=32,\n",
    "                images_dir=None, vectorized_dir=None, lr=1e-2, patience=4, delete_folder=False,\n",
    "                occlusion_augmentation=False, elastic_augmentation=False, shuffle_augmentation=None):\n",
    "    \"\"\"\n",
    "    Trains a CNN using compressed whole-slide images.\n",
    "\n",
    "    :param featurized_dir: folder containing the compressed (featurized) images.\n",
    "    :param csv_path: list of slides with labels.\n",
    "    :param fold_n: fold determining which data partitions to use for training, validation and testing.\n",
    "    :param output_dir: destination folder to store results.\n",
    "    :param cache_dir: folder to store compressed images temporarily for fast access.\n",
    "    :param batch_size: number of samples to train with in one-go.\n",
    "    :return: nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Delete folder and subfolders if exists\n",
    "    if delete_folder:\n",
    "        if exists(result_dir):  shutil.rmtree(result_dir)\n",
    "\n",
    "    # Train CNN\n",
    "    train_wsi_classifier(\n",
    "        data_dir=featurized_dir,\n",
    "        csv_path=csv_path,\n",
    "        partitions=None,\n",
    "        crop_size=400,\n",
    "        output_dir=output_dir,\n",
    "        output_units=2,\n",
    "        cache_dir=cache_dir,\n",
    "        n_epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        code_size=128,\n",
    "        workers=1,\n",
    "        train_step_multiplier=1,\n",
    "        val_step_multiplier=0.5,\n",
    "        keep_data_training=1,\n",
    "        keep_data_validation=1,\n",
    "        patience=patience,\n",
    "        occlusion_augmentation=occlusion_augmentation,\n",
    "        elastic_augmentation=elastic_augmentation,\n",
    "        shuffle_augmentation=shuffle_augmentation\n",
    "    )\n",
    "\n",
    "    # Evaluate CNN\n",
    "\n",
    "    # Get compressed wsi directories with csv test file\n",
    "    data_config = featurized_dir\n",
    "    data_config['csv_path'] = csv_path['csv_test']\n",
    "\n",
    "    eval_model(\n",
    "        model_path=join(output_dir, 'checkpoint.h5'),\n",
    "        data_config=data_config,\n",
    "        crop_size=400,\n",
    "        output_path=join(output_dir, 'eval', 'preds.csv'),\n",
    "        cache_dir=None,\n",
    "        batch_size=batch_size,\n",
    "        keep_data=1\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    try:\n",
    "        compute_metrics(\n",
    "            input_path=join(output_dir, 'eval', 'preds.csv'),\n",
    "            output_dir=join(output_dir, 'eval')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to compute metrics. Exception: {e}'.format(e=e), flush=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating compressed wsi csv file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1593/1593 [00:00<00:00, 309688.36it/s]\n",
      "100%|██████████| 1518/1518 [00:00<00:00, 262025.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv file sucessfully exported!\n",
      "Creating split train/validation/test csv files with no augmentations ...\n",
      "Train/validation/test csv files sucessfully exported!\n",
      "Files were read with shapes: Training: (580, 2), Validation (249, 2), Testing (208, 2)\n",
      "Total files: Files were read with shapes: 1037\n",
      "Loading training set ...\n",
      "FeaturizedWsiGenerator data config: {'data_dir_luad': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_luad/featurized/no_augmentations', 'data_dir_lusc': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_lusc/featurized/no_augmentations', 'csv_path': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/data/train_slide_list_tcga.csv'}\n",
      "FeaturizedWsiGenerator using 580 samples and 19 batches, distributed in 306 positive and 274 negative samples.\n",
      "Loading validation set ...\n",
      "FeaturizedWsiSequence data config: {'data_dir_luad': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_luad/featurized/no_augmentations', 'data_dir_lusc': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/tcga_lusc/featurized/no_augmentations', 'csv_path': '/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/data/validation_slide_list_tcga.csv'}\n",
      "FeaturizedWsiSequence using 249 samples and 8 batches, distributed in 128 positive and 121 negative samples.\n",
      "Building model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ...\n",
      "Training model in directory: /mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction/results/model with content 0\n",
      "Training model from scratch False False...\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400, 400, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 199, 199, 128)     17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 199, 199, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 199, 199, 128)     0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 199, 199, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 99, 99, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 99, 99, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 49, 49, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 49, 49, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 24, 24, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 11, 11, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_5 (Spatial (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 5, 5, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_6 (Spatial (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_7 (Separabl (None, 3, 3, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_7 (Spatial (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_8 (Separabl (None, 1, 1, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_8 (Spatial (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 162,690\n",
      "Trainable params: 160,386\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Creates csv from original data\n",
    "\n",
    "# project and data directories\n",
    "root_dir = r'/mnt/netcache/pathology/projects/pathology-weakly-supervised-lung-cancer-growth-pattern-prediction'\n",
    "data_dir = r'/mnt/netcache/pathology/archives/lung'\n",
    "\n",
    "# wsi directories\n",
    "dir_luad_wsi = os.path.join(data_dir, 'TCGA_LUAD', 'wsi_diagnostic_tif')\n",
    "dir_lusc_wsi = os.path.join(data_dir, 'TCGA_LUSC', 'wsi_diagnostic_tif')\n",
    "dir_luad_wsi_mask = os.path.join(data_dir, 'TCGA_LUAD', 'tissue_masks_diagnostic')\n",
    "dir_lusc_wsi_mask = os.path.join(data_dir, 'TCGA_LUSC', 'tissue_masks_diagnostic')\n",
    "\n",
    "# compressed image directories\n",
    "vectorized_luad_dir = join(root_dir, 'results', 'tcga_luad', 'vectorized')\n",
    "vectorized_lusc_dir = join(root_dir, 'results', 'tcga_lusc', 'vectorized')\n",
    "featurized_luad_dir = join(root_dir, 'results', 'tcga_luad', 'featurized', 'no_augmentations')\n",
    "featurized_lusc_dir = join(root_dir, 'results', 'tcga_lusc', 'featurized', 'no_augmentations')\n",
    "\n",
    "# results directory\n",
    "result_dir = join(root_dir, 'results', 'model')  # store the results from trained model\n",
    "gradcam_dir = join(result_dir, 'gradcam')  # store gradcam results\n",
    "\n",
    "# Set paths\n",
    "model_path = './neural-image-compression-private/models/encoders_patches_pathology/encoder_bigan.h5'\n",
    "csv_train = os.path.join(root_dir, 'data', 'train_slide_list_tcga.csv')\n",
    "csv_val = os.path.join(root_dir, 'data', 'validation_slide_list_tcga.csv')\n",
    "csv_test = os.path.join(root_dir, 'data', 'test_slide_list_tcga.csv')\n",
    "csv_path_luad_feat = join(root_dir, 'data', 'slide_list_featurized_luad.csv')\n",
    "\n",
    "# csv paths\n",
    "csv_path_wsi = os.path.join(root_dir, 'data', 'slide_original_list_tcga.csv')\n",
    "csv_path_compressed_wsi = os.path.join(root_dir, 'data', 'slide_compressed_list_tcga.csv')\n",
    "\n",
    "cache_dir = None  # used to store local copies of files during I/O operations (useful in cluster\n",
    "\n",
    "\n",
    "# Train CNN\n",
    "\n",
    "# selected_fold = 0\n",
    "\n",
    "featurized_dir = {'data_dir_luad': featurized_luad_dir, 'data_dir_lusc': featurized_lusc_dir}\n",
    "csv_path = {'csv_train': csv_train, 'csv_val': csv_val, 'csv_test': csv_test}\n",
    "\n",
    "\n",
    "# Create csv files \n",
    "print('Creating compressed wsi csv file ...')\n",
    "create_csv(featurized_luad_dir, featurized_lusc_dir, csv_path_compressed_wsi)\n",
    "\n",
    "print('Creating split train/validation/test csv files with no augmentations ...')\n",
    "generate_csv_files(csv_path_compressed_wsi, csv_train, csv_val, csv_test, test_size=0.2, validation_size = 0.3)\n",
    "\n",
    "# read files to check shapes\n",
    "df = pd.read_csv(csv_train);  df2 = pd.read_csv(csv_val);   df3 = pd.read_csv(csv_test)\n",
    "print(f'Files were read with shapes: Training: {df.shape}, Validation {df2.shape}, Testing {df3.shape}')\n",
    "print(f'Total files: Files were read with shapes: {df.shape[0]+df2.shape[0]+df3.shape[0]}')\n",
    "\n",
    "train_model(\n",
    "    featurized_dir=featurized_dir,\n",
    "    csv_path=csv_path,\n",
    "    fold_n=0,\n",
    "    output_dir=result_dir,\n",
    "    cache_dir=None,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    delete_folder=True,\n",
    "    occlusion_augmentation=False,\n",
    "    lr=1e-2,\n",
    "    patience=4,\n",
    "    elastic_augmentation=False,\n",
    "    images_dir=None,  # required for GradCAM\n",
    "    vectorized_dir=None,  # required for GradCAM\n",
    "    shuffle_augmentation=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GradCam will be apply to this dataset!')\n",
    "\n",
    "data_to_csv(featurized_luad_dir, csv_path_luad_feat)\n",
    "pd.read_csv(csv_path_luad_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GradCam on layer 1\n",
    "gradcam_on_dataset(\n",
    "    data_dir=[featurized_luad_dir, featurized_lusc_dir],\n",
    "    csv_path=csv_path_luad_feat,\n",
    "    model_path=join(result_dir, 'checkpoint.h5'),\n",
    "    partitions=0,\n",
    "    layer_number=1,\n",
    "    custom_objects=None,\n",
    "    cache_dir=cache_dir,\n",
    "    images_dir=dir_luad_wsi,\n",
    "    vectorized_dir=vectorized_luad_dir,\n",
    "    output_dir=gradcam_dir,\n",
    "    predict_two_output = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
