{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy data to local instance\n",
    "\n",
    "import os\n",
    "\n",
    "os.system('mkdir tcga_luad/')\n",
    "os.system('mkdir tcga_lusc/')\n",
    "os.system('cp /mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/tcga/featurized/tcga_luad/normal/* ./tcga_luad') \n",
    "os.system('cp /mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/tcga/featurized/tcga_lusc/normal/* ./tcga_lusc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set ...\n",
      "FeaturizedWsiGenerator data config: {'data_dir_luad': '/home/user/tcga_luad', 'data_dir_lusc': '/home/user/tcga_lusc', 'csv_path': '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/data/tcga/slide_list_tcga.csv'}\n",
      "FeaturizedWsiGenerator using 1037 samples and 130 batches, distributed in 531 positive and 506 negative samples.\n",
      "Loading validation set ...\n",
      "FeaturizedWsiSequence data config: {'data_dir_luad': '/home/user/tcga_luad', 'data_dir_lusc': '/home/user/tcga_lusc', 'csv_path': '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/data/tcga/slide_list_tcga.csv'}\n",
      "FeaturizedWsiSequence using 1037 samples and 133 batches, distributed in 531 positive and 506 negative samples.\n",
      "Building model ...\n",
      "Training model ...\n",
      "Training model in directory: /mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/model with content 0\n",
      "Training model from scratch False False...\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400, 400, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 199, 199, 128)     17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 199, 199, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 199, 199, 128)     0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 199, 199, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 99, 99, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 99, 99, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 49, 49, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 49, 49, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 24, 24, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 11, 11, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_5 (Spatial (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 5, 5, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_6 (Spatial (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_7 (Separabl (None, 3, 3, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_7 (Spatial (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_8 (Separabl (None, 1, 1, 128)         17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_8 (Spatial (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 162,690\n",
      "Trainable params: 160,386\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "130/130 [==============================] - 285s 2s/step - loss: 0.8699 - categorical_accuracy: 0.5183 - val_loss: 0.7686 - val_categorical_accuracy: 0.5417\n",
      "Epoch 00000: val_loss improved from inf to 0.76855, saving model to /mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/model/checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/code/neural-image-compression-private/source/nic/callbacks.py:368: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  current = df.ix[len(df) - 1, self.monitor]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to /mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/model/last_epoch.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/code/neural-image-compression-private/source/nic/callbacks.py:233: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  current = df.ix[len(df) - 1, self.monitor]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      " 53/130 [===========>..................] - ETA: 1:29 - loss: 0.7393 - categorical_accuracy: 0.5425"
     ]
    }
   ],
   "source": [
    "# Import NIC to python path\n",
    "import sys\n",
    "nic_dir = '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/code/neural-image-compression-private'\n",
    "sys.path.append(nic_dir +'/source')\n",
    "\n",
    "\"\"\"\n",
    "Train a CNN on compressed whole-slide images.\n",
    "\n",
    "    class 1 : luad\n",
    "    class 0 : lusc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "# from nic.gradcam_wsi import gradcam_on_dataset\n",
    "# from Project.train_compressed_wsi import FeaturizedWsiGenerator, FeaturizedWsiSequence\n",
    "# from digitalpathology.image.io import imagereader\n",
    "import scipy\n",
    "from nic.util_fns import cache_file\n",
    "import glob\n",
    "from os.path import exists, join, basename\n",
    "import shutil\n",
    "from nic.callbacks import ReduceLROnPlateau, ModelCheckpoint, HistoryCsv, FinishedFlag, PlotHistory, StoreModelSummary, \\\n",
    "    CopyResultsExternally, LearningRateScheduler\n",
    "\n",
    "\"\"\"\n",
    "531 in LUAD\n",
    "506 in LUSC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_csv(data_paths):\n",
    "    \"\"\"\n",
    "    Output: csv file with slide names and corresponding labels, to be use for preprocessing\n",
    "        labels 1 correspond to LUAD\n",
    "        labesl 0 correspond to LUSC\n",
    "    \"\"\"\n",
    "\n",
    "    lusc_dir = data_paths['data_dir_lusc']\n",
    "    luad_dir = data_paths['data_dir_luad']\n",
    "    csv_path = data_paths['csv_path']\n",
    "\n",
    "    lusc_paths = sorted(\n",
    "        [(os.path.basename(file)).split('.')[0] for file in os.listdir(lusc_dir) if file.endswith('.png')])\n",
    "    luad_paths = sorted(\n",
    "        [(os.path.basename(file)).split('.')[0] for file in os.listdir(luad_dir) if file.endswith('.png')])\n",
    "    luad_labels = np.ones(len(luad_paths), dtype=np.int8)\n",
    "    lusc_labels = np.zeros(len(lusc_paths), dtype=np.int8)\n",
    "\n",
    "    lusc_df = pd.DataFrame(list(zip(lusc_paths, lusc_labels)), columns=['slide_id', 'label'])\n",
    "    luad_df = pd.DataFrame(list(zip(luad_paths, luad_labels)), columns=['slide_id', 'label'])\n",
    "\n",
    "    # conacatenate dataframes\n",
    "    data = pd.concat([lusc_df, luad_df], ignore_index=True, )\n",
    "    export_csv = data.to_csv(csv_path, index=None, header=True)\n",
    "    print('Csv file sucessfully exported!')\n",
    "\n",
    "#data_config = {'data_dir_luad': data_dir_luad, 'data_dir_lusc': data_dir_lusc, 'csv_path': csv_path}\n",
    "\n",
    "#create_csv(data_config)\n",
    "\n",
    "def get_labels_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = shuffle(df)\n",
    "    image_ids = list(df['slide_id'].values)\n",
    "    labels = df['label'].values.astype('uint8')\n",
    "\n",
    "    return image_ids, labels\n",
    "\n",
    "\n",
    "def read_data(data_conf):\n",
    "    \"\"\"\n",
    "    Data reader\n",
    "    Inputs:\n",
    "        data_conf : dictionary with the data paths (featurize paths and csv file)\n",
    "    Outputs:\n",
    "        image_ids_all, features_path, distance_map_path, labels_all, features_ids_all\n",
    "    \"\"\"\n",
    "\n",
    "    # Get params\n",
    "    data_dir_class0 = data_conf['data_dir_lusc']\n",
    "    data_dir_class1 = data_conf['data_dir_luad']\n",
    "    csv_dir = data_conf['csv_path']\n",
    "\n",
    "    # Read image file names\n",
    "    df = pd.read_csv(csv_dir)\n",
    "    df = shuffle(df)\n",
    "    image_ids = list(df['slide_id'].values)\n",
    "    labels = df['label'].values.astype('uint8')\n",
    "\n",
    "    # Get paths\n",
    "    image_ids_all = [];\n",
    "    features_path = [];\n",
    "    distance_map_path = [];\n",
    "    labels_all = [];\n",
    "    features_ids_all = []\n",
    "\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        label = labels[i]\n",
    "        if label == 0:\n",
    "            f_path = os.path.join(data_dir_class0, '{image_id}.npy'.format(image_id=image_id))\n",
    "            dm_path = os.path.join(data_dir_class0, '{image_id}_distance_map.npy'.format(image_id=image_id))\n",
    "            feature_id = os.path.splitext(os.path.basename(f_path))[0][:-9]\n",
    "        else:\n",
    "            f_path = os.path.join(data_dir_class1, '{image_id}.npy'.format(image_id=image_id))\n",
    "            dm_path = os.path.join(data_dir_class1, '{image_id}_distance_map.npy'.format(image_id=image_id))\n",
    "            feature_id = os.path.splitext(os.path.basename(f_path))[0][:-9]\n",
    "        image_ids_all.append(image_id)\n",
    "        features_path.append(f_path)\n",
    "        distance_map_path.append(dm_path)\n",
    "        labels_all.append(label)\n",
    "        features_ids_all.append(feature_id)\n",
    "\n",
    "    # Shuffle\n",
    "    idx = np.random.choice(len(image_ids_all), len(image_ids_all), replace=False)\n",
    "    image_ids_all = [image_ids_all[i] for i in idx]\n",
    "    features_path = [features_path[i] for i in idx]\n",
    "    distance_map_path = [distance_map_path[i] for i in idx]\n",
    "    features_ids_all = [features_ids_all[i] for i in idx]\n",
    "    labels_all = np.array([labels_all[i] for i in idx]).astype('uint8')\n",
    "\n",
    "    return image_ids_all, features_path, distance_map_path, labels_all, features_ids_all\n",
    "\n",
    "\n",
    "def crop_features(features, distance_map, crop_size, deterministic=False, crop_id=None, n_crops=None):\n",
    "    # Sample center\n",
    "    if deterministic:\n",
    "        if (crop_id is not None) and (n_crops is not None):\n",
    "            distance_map_idxs = np.where(distance_map.flatten() != 0)[0]\n",
    "            center = distance_map_idxs[int(len(distance_map_idxs) * (crop_id / n_crops))]\n",
    "        else:\n",
    "            center = np.argmax(distance_map.flatten())\n",
    "        center = np.unravel_index(center, distance_map.shape)\n",
    "        x_center, y_center = center\n",
    "    else:\n",
    "        center = np.random.choice(len(distance_map.flatten()), 1, replace=True, p=distance_map.flatten())\n",
    "        center = np.unravel_index(center, distance_map.shape)\n",
    "        x_center, y_center = (center[0][0], center[1][0])\n",
    "\n",
    "    # Crop params\n",
    "    x_size = features.shape[0]\n",
    "    y_size = features.shape[1]\n",
    "    x1 = x_center - crop_size // 2\n",
    "    x2 = x_center + crop_size // 2\n",
    "    y1 = y_center - crop_size // 2\n",
    "    y2 = y_center + crop_size // 2\n",
    "\n",
    "    # Pad\n",
    "    padx1 = np.abs(np.min([0, x1]))\n",
    "    padx2 = np.abs(np.min([0, x_size - x2]))\n",
    "    pady1 = np.abs(np.min([0, y1]))\n",
    "    pady2 = np.abs(np.min([0, y_size - y2]))\n",
    "    padding = ((padx1, padx2), (pady1, pady2), (0, 0))\n",
    "    features = np.pad(features, padding, 'constant')\n",
    "    x1 += padx1\n",
    "    x2 += padx1\n",
    "    y1 += pady1\n",
    "    y2 += pady1\n",
    "\n",
    "    # Crop\n",
    "    features = features[x1:x2, y1:y2, :]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class FeaturizedWsiGenerator(object):\n",
    "\n",
    "    def __init__(self, data_config, data_fn, batch_size, augment, crop_size, cache_dir=None, balanced=True,\n",
    "                 keep_data=1.0, occlusion_augmentation=False, elastic_augmentation=False, shuffle_augmentation=None,\n",
    "                 binary_target=True):\n",
    "\n",
    "        # Params\n",
    "        self.batch_size = batch_size\n",
    "        self.data_config = data_config\n",
    "        self.augment = augment\n",
    "        self.crop_size = crop_size\n",
    "        self.cache_dir = cache_dir\n",
    "        self.balanced = balanced\n",
    "        self.keep_data = keep_data\n",
    "        self.occlusion_augmentation = occlusion_augmentation\n",
    "        self.elastic_augmentation = elastic_augmentation\n",
    "        self.shuffle_augmentation = shuffle_augmentation\n",
    "        self.binary_target = binary_target\n",
    "\n",
    "        # Cache dir\n",
    "        if self.cache_dir and not exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "\n",
    "        # Read paths\n",
    "        self.image_ids, self.paths, self.dm_paths, self.labels, self.feature_ids = data_fn(data_config)\n",
    "\n",
    "        # Keep data (assume they are shuffled)\n",
    "        n = int(len(self.image_ids) * keep_data)\n",
    "        self.image_ids = self.image_ids[:n]\n",
    "        self.paths = self.paths[:n]\n",
    "        self.dm_paths = self.dm_paths[:n] if self.dm_paths is not None else None\n",
    "        self.labels = self.labels[:n]\n",
    "        self.feature_ids = self.feature_ids[:n]\n",
    "\n",
    "        # Indexes for positive and negative samples\n",
    "        if self.balanced and self.binary_target:\n",
    "            self.pos_idx = np.where(self.labels == 1)[0]\n",
    "            self.neg_idx = np.where(self.labels == 0)[0]\n",
    "        else:\n",
    "            self.pos_idx = np.arange(len(self.paths))\n",
    "            self.neg_idx = np.arange(len(self.paths))\n",
    "\n",
    "        # Other\n",
    "        self.n_samples = len(self.paths)\n",
    "        self.n_batches = int(np.ceil(self.n_samples / batch_size))\n",
    "\n",
    "        # Print\n",
    "        print('FeaturizedWsiGenerator data config: ' + str(data_config), flush=True)\n",
    "        print(\n",
    "            'FeaturizedWsiGenerator using {n1} samples and {n2} batches, distributed in {n3} positive and {n4} negative samples.'.format(\n",
    "                n1=self.n_samples, n2=self.n_batches, n3=len(self.pos_idx), n4=len(self.neg_idx)\n",
    "            ), flush=True)\n",
    "\n",
    "        # Elastic\n",
    "        if self.elastic_augmentation:\n",
    "            self.n_maps = 50\n",
    "            self.deformation_maps = self.create_deformation_maps()\n",
    "        else:\n",
    "            self.n_maps = None\n",
    "            self.deformation_maps = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Provide length in number of batches\n",
    "        Returns (int): number of batches available in the entire dataset.\n",
    "        \"\"\"\n",
    "        return self.n_batches\n",
    "\n",
    "    def create_deformation_maps(self):\n",
    "\n",
    "        alpha_interval = (10000, 50000)  # (300, 1200) # TODO test higher values!\n",
    "        sigma_interval = (20.0, 20.0)\n",
    "        image_shape = (self.crop_size, self.crop_size, 1)\n",
    "        deformation_maps = []\n",
    "\n",
    "        for _ in range(self.n_maps):\n",
    "            alpha = np.random.uniform(low=alpha_interval[0], high=alpha_interval[1], size=None)\n",
    "            sigma = np.random.uniform(low=sigma_interval[0], high=sigma_interval[1], size=None)\n",
    "\n",
    "            dx = scipy.ndimage.filters.gaussian_filter(input=(np.random.rand(*image_shape) * 2 - 1), sigma=sigma,\n",
    "                                                       mode='constant', cval=0) * alpha\n",
    "            dy = scipy.ndimage.filters.gaussian_filter(input=(np.random.rand(*image_shape) * 2 - 1), sigma=sigma,\n",
    "                                                       mode='constant', cval=0) * alpha\n",
    "            z, x, y = np.meshgrid(np.arange(image_shape[0]), np.arange(image_shape[1]), np.arange(image_shape[2]),\n",
    "                                  indexing='ij')\n",
    "            indices = (np.reshape(z, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)))\n",
    "\n",
    "            deformation_maps.append(indices)\n",
    "\n",
    "        return deformation_maps\n",
    "\n",
    "    def augment_batch(self, x, y):\n",
    "        \"\"\"\n",
    "        Randomly applies 90-degree rotation and horizontal-vertical flipping (same augmentation for the entire batch).\n",
    "\n",
    "        Args:\n",
    "            x: batch of images with shape [batch, x, y, c].\n",
    "\n",
    "        Returns: batch of augmented images.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Flip\n",
    "        x = np.flip(x, np.random.randint(2) + 1)\n",
    "\n",
    "        # Rot\n",
    "        x = np.rot90(x, np.random.randint(4), axes=(1, 2))\n",
    "\n",
    "        # Elastic\n",
    "        if self.elastic_augmentation:\n",
    "\n",
    "            # Per sample\n",
    "            for i in range(len(x)):\n",
    "                if np.random.rand() > 0.25:\n",
    "\n",
    "                    indices = self.deformation_maps[np.random.randint(0, self.n_maps)]\n",
    "\n",
    "                    # Per channel\n",
    "                    for j in range(x.shape[-1]):\n",
    "                        x[i, :, :, j] = scipy.ndimage.interpolation.map_coordinates(\n",
    "                            input=x[i, :, :, j:j + 1], coordinates=indices, order=0,\n",
    "                            mode='reflect').reshape(x[i, :, :, j].shape)\n",
    "\n",
    "        # Shuffle crop augmentation\n",
    "        if self.shuffle_augmentation is not None:\n",
    "\n",
    "            # from featurize_wsi import plot_feature_map\n",
    "\n",
    "            # Per sample\n",
    "            labels = np.argmax(y, axis=-1)\n",
    "            x_source = np.copy(x)\n",
    "            for i in range(len(x)):\n",
    "                # plot_feature_map(np.copy(x[i].transpose((2, 0, 1))), r'W:\\projects\\pathology-liver-survival\\debug\\shuffle_augmentation\\{i}_before.png'.format(i=i)) # todo\n",
    "\n",
    "                # Find target\n",
    "                idxs = np.random.choice(np.where(labels == labels[i])[0], self.shuffle_augmentation)\n",
    "                for idx in idxs:\n",
    "                    # Crop coordinates\n",
    "                    x1 = int(np.random.uniform(0, self.crop_size))\n",
    "                    y1 = int(np.random.uniform(0, self.crop_size))\n",
    "\n",
    "                    # Paste\n",
    "                    x[i, x1:, y1:, :] = x_source[idx, x1:, y1:, :]\n",
    "\n",
    "                    # Rotate\n",
    "                    x[i, ...] = np.rot90(x[i, ...], np.random.randint(4), axes=(0, 1))\n",
    "\n",
    "                # plot_feature_map(np.copy(x[i].transpose((2, 0, 1))), r'W:\\projects\\pathology-liver-survival\\debug\\shuffle_augmentation\\{i}_after.png'.format(i=i))\n",
    "\n",
    "        # Occlusion\n",
    "        if self.occlusion_augmentation:\n",
    "\n",
    "            # Per sample\n",
    "            for i in range(len(x)):\n",
    "                if np.random.rand() > 0.25:\n",
    "                    x1 = int(np.random.uniform(0, self.crop_size // 2))\n",
    "                    y1 = int(np.random.uniform(0, self.crop_size // 2))\n",
    "                    x2 = int(np.random.uniform(self.crop_size // 2, self.crop_size))\n",
    "                    y2 = int(np.random.uniform(self.crop_size // 2, self.crop_size))\n",
    "                    x[i, x1:x1 + x2, y1:y1 + y2, :] = 0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def assemble_batch(self, idxs):\n",
    "        \"\"\"\n",
    "        Creates a training batch from featurized WSIs on disk. It samples a crop taking distance to background into\n",
    "        account (crops centered on lots of tissue are more likely). It pads the crops if needed. It copies the files\n",
    "        to cache if needed.\n",
    "\n",
    "        Args:\n",
    "            idxs: file indexes to process.\n",
    "\n",
    "        Returns: tuple of batch and labels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for idx in idxs:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Get features\n",
    "                if self.cache_dir:\n",
    "                    self.paths[idx] = cache_file(self.paths[idx], self.cache_dir, overwrite=False)\n",
    "                features = np.load(self.paths[idx]).astype('float32').transpose((1, 2, 0))\n",
    "\n",
    "                # Get distance map\n",
    "                if self.dm_paths is not None:\n",
    "                    self.dm_paths[idx] = cache_file(self.dm_paths[idx], self.cache_dir, overwrite=False)\n",
    "                    distance_map = np.load(self.dm_paths[idx])\n",
    "\n",
    "                    # Crop\n",
    "                    features = crop_features(features, distance_map, crop_size=self.crop_size, deterministic=False)\n",
    "\n",
    "                # Append\n",
    "                x.append(features)\n",
    "\n",
    "                # Label\n",
    "                y.append(self.labels[idx])\n",
    "\n",
    "                # print('File {f} with label {l}'.format(f=basename(self.paths[idx]), l=self.labels[idx]), flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    'FeaturizedWsiGenerator failed to assemble batch with idx {idx}, skipping sample. Exception: {e}'.format(\n",
    "                        idx=idx, e=e), flush=True)\n",
    "\n",
    "        # Fill\n",
    "        if len(x) < len(idxs):\n",
    "            print('Filling batch to match batch size...', flush=True)\n",
    "            fill_idxs = np.random.choice(len(x), len(idxs) - len(x), replace=False).astype('uint8')\n",
    "            for fill_idx in fill_idxs:\n",
    "                x.append(x[fill_idx])\n",
    "                y.append(y[fill_idx])\n",
    "\n",
    "        # Concat\n",
    "        x = np.stack(x, axis=0)\n",
    "        if self.binary_target:\n",
    "            y = np.eye(2)[np.array(y, dtype='int')]\n",
    "        else:\n",
    "            y = np.array(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Builds the training batch.\n",
    "\n",
    "        Returns: tuple of batch and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get samples\n",
    "        idxs_pos = np.random.choice(self.pos_idx, self.batch_size // 2, replace=True)\n",
    "        idxs_neg = np.random.choice(self.neg_idx, self.batch_size // 2, replace=True)\n",
    "\n",
    "        # Merge\n",
    "        idxs = np.concatenate([idxs_pos, idxs_neg])\n",
    "\n",
    "        # Randomize\n",
    "        r = np.random.choice(len(idxs), len(idxs), replace=False)\n",
    "        idxs = idxs[r]\n",
    "\n",
    "        # Build batch\n",
    "        x, y = self.assemble_batch(idxs)\n",
    "\n",
    "        # Augment\n",
    "        if self.augment:\n",
    "            x = self.augment_batch(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class FeaturizedWsiSequence(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Class to randomly provide batches of featurized WSIs loaded from numpy arrays on disk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_config, data_fn, batch_size, crop_size, balanced, cache_dir=None, keep_data=1.0,\n",
    "                 return_ids=False, binary_target=True, n_crops=None):\n",
    "\n",
    "        # Params\n",
    "        self.batch_size = batch_size\n",
    "        self.data_config = data_config\n",
    "        self.crop_size = crop_size\n",
    "        self.cache_dir = cache_dir\n",
    "        self.keep_data = keep_data\n",
    "        self.balanced = balanced\n",
    "        self.return_ids = return_ids\n",
    "        self.binary_target = binary_target\n",
    "        self.n_crops = n_crops\n",
    "\n",
    "        # Cache dir\n",
    "        if self.cache_dir and not exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "\n",
    "        # Read paths\n",
    "        self.image_ids, self.paths, self.dm_paths, self.labels, self.feature_ids = data_fn(data_config)\n",
    "\n",
    "        # Keep data (assume they are shuffled)\n",
    "        n = int(np.ceil(len(self.image_ids) * keep_data))\n",
    "        self.image_ids = self.image_ids[:n]\n",
    "        self.paths = self.paths[:n]\n",
    "        self.dm_paths = self.dm_paths[:n] if self.dm_paths is not None else None\n",
    "        self.labels = self.labels[:n]\n",
    "        self.feature_ids = self.feature_ids[:n]\n",
    "\n",
    "        # N crops\n",
    "        if self.n_crops is not None:\n",
    "\n",
    "            # Extend set\n",
    "            def extend(l, n):\n",
    "                nl = []\n",
    "                for i in l:\n",
    "                    for j in range(n):\n",
    "                        nl.append(i)\n",
    "                return nl\n",
    "\n",
    "            self.crop_ids = np.concatenate([np.arange(n_crops) for _ in self.image_ids])\n",
    "            self.image_ids = extend(self.image_ids, n_crops)\n",
    "            self.paths = extend(self.paths, n_crops)\n",
    "            self.dm_paths = extend(self.dm_paths, n_crops)\n",
    "            self.labels = extend(self.labels, n_crops)\n",
    "            self.feature_ids = extend(self.feature_ids, n_crops)\n",
    "        else:\n",
    "            self.crop_ids = None\n",
    "\n",
    "        # Indexes for positive and negative samples\n",
    "        if self.balanced and self.binary_target and self.n_crops is None:\n",
    "            self.pos_idx = np.where(self.labels == 1)[0]\n",
    "            self.neg_idx = np.where(self.labels == 0)[0]\n",
    "        else:\n",
    "            self.pos_idx = np.arange(len(self.paths))\n",
    "            self.neg_idx = np.arange(len(self.paths))\n",
    "\n",
    "        # Other\n",
    "        if self.balanced and self.binary_target and self.n_crops is None:\n",
    "            self.n_batches = int(np.ceil(np.max([len(self.pos_idx), len(self.neg_idx)]) * 2 / batch_size))\n",
    "        else:\n",
    "            self.n_batches = int(np.ceil(len(self.labels) / batch_size))\n",
    "\n",
    "        # Print\n",
    "        print('FeaturizedWsiSequence data config: ' + str(data_config), flush=True)\n",
    "        print(\n",
    "            'FeaturizedWsiSequence using {n1} samples and {n2} batches, distributed in {n3} positive and {n4} negative samples.'.format(\n",
    "                n1=len(self.image_ids), n2=self.n_batches, n3=len(self.pos_idx), n4=len(self.neg_idx)\n",
    "            ), flush=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Provide length in number of batches\n",
    "        Returns (int): number of batches available in the entire dataset.\n",
    "        \"\"\"\n",
    "        return self.n_batches\n",
    "\n",
    "    def assemble_batch(self, idxs):\n",
    "        \"\"\"\n",
    "        Creates a batch from featurized WSIs on disk. It samples a crop taking the center with the maximum distance to\n",
    "        background. It pads the crops if needed. It copies the files to cache if needed.\n",
    "\n",
    "        Args:\n",
    "            idxs: file indexes to process.\n",
    "\n",
    "        Returns: tuple of batch and labels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        ids = []\n",
    "        for idx in idxs:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Get features\n",
    "                if self.cache_dir:\n",
    "                    self.paths[idx] = cache_file(self.paths[idx], self.cache_dir, overwrite=False)\n",
    "                features = np.load(self.paths[idx]).astype('float32').transpose((1, 2, 0))\n",
    "\n",
    "                # Get distance map\n",
    "                if self.dm_paths is not None:\n",
    "                    self.dm_paths[idx] = cache_file(self.dm_paths[idx], self.cache_dir, overwrite=False)\n",
    "                    distance_map = np.load(self.dm_paths[idx])\n",
    "\n",
    "                    # Crop\n",
    "                    if self.n_crops is not None:\n",
    "                        features = crop_features(features, distance_map, crop_size=self.crop_size, deterministic=True,\n",
    "                                                 crop_id=self.crop_ids[idx], n_crops=self.n_crops)\n",
    "                    else:\n",
    "                        features = crop_features(features, distance_map, crop_size=self.crop_size, deterministic=True)\n",
    "\n",
    "                # Get ids\n",
    "                ids.append(self.feature_ids[idx])\n",
    "\n",
    "                # Append\n",
    "                x.append(features)\n",
    "\n",
    "                # Label\n",
    "                y.append(self.labels[idx])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    'FeaturizedWsiSequence failed to assemble batch with idx {idx}, skipping sample. Exception: {e}'.format(\n",
    "                        idx=idx, e=e), flush=True)\n",
    "\n",
    "        # Fill\n",
    "        if len(x) < len(idxs):\n",
    "            print('Filling batch to match batch size...', flush=True)\n",
    "            fill_idxs = np.zeros(len(idxs) - len(x), dtype='uint8')  # fill with first sample\n",
    "            for fill_idx in fill_idxs:\n",
    "                x.append(x[fill_idx])\n",
    "                y.append(y[fill_idx])\n",
    "                ids.append(ids[fill_idx])\n",
    "\n",
    "        # Concat\n",
    "        x = np.stack(x, axis=0)\n",
    "        y = np.array(y).astype('float')\n",
    "        y_na = np.copy(y)\n",
    "        y[np.isnan(y)] = 0\n",
    "        if self.binary_target:\n",
    "            y = np.eye(2)[np.array(y, dtype='int')]\n",
    "        y[np.isnan(y_na), ...] = np.nan\n",
    "\n",
    "        return x, y, ids\n",
    "\n",
    "    def get_idxs(self, idx):\n",
    "\n",
    "        if self.balanced and self.binary_target and self.n_crops is None:\n",
    "            # Get positive samples\n",
    "            idx_batch_pos = idx * self.batch_size // 2\n",
    "            idxs_pos = np.mod(np.arange(idx_batch_pos, idx_batch_pos + self.batch_size // 2), len(self.pos_idx))\n",
    "            idxs_pos = self.pos_idx[idxs_pos]\n",
    "\n",
    "            # Get negative samples\n",
    "            idx_batch_neg = idx * self.batch_size // 2\n",
    "            idxs_neg = np.mod(np.arange(idx_batch_neg, idx_batch_neg + self.batch_size // 2), len(self.neg_idx))\n",
    "            idxs_neg = self.neg_idx[idxs_neg]\n",
    "\n",
    "            idxs = np.concatenate([idxs_pos, idxs_neg])\n",
    "\n",
    "        else:\n",
    "            # Get samples\n",
    "            idx_batch = idx * self.batch_size\n",
    "            if idx_batch + self.batch_size >= len(self.labels):\n",
    "                idxs = np.arange(idx_batch, len(self.labels))\n",
    "            else:\n",
    "                idxs = np.arange(idx_batch, idx_batch + self.batch_size)\n",
    "\n",
    "        return idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Builds the batch (balanced if needed).\n",
    "\n",
    "        Returns: tuple of batch and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Find idxs\n",
    "        idxs = self.get_idxs(idx)\n",
    "\n",
    "        # Build batch\n",
    "        x, y, ids = self.assemble_batch(idxs)\n",
    "\n",
    "        if self.return_ids:\n",
    "            return x, y, ids\n",
    "        else:\n",
    "            return x, y\n",
    "\n",
    "\n",
    "def build_wsi_classifier(input_shape, lr, output_units):\n",
    "    \"\"\"\n",
    "    Builds a neural network that performs classification on featurized WSIs.\n",
    "\n",
    "    Args:\n",
    "        input_shape: shape of features with channels last, for example (400, 400, 128).\n",
    "        lr (float): learning rate.\n",
    "\n",
    "    Returns: compiled Keras model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def conv_op(x, stride, dropout=0.2):\n",
    "\n",
    "        # Conv\n",
    "        l2_reg = keras.regularizers.l2(1e-5)\n",
    "        x = keras.layers.SeparableConv2D(\n",
    "            filters=128, kernel_size=3, strides=stride, padding='valid', depth_multiplier=1,\n",
    "            activation='linear', depthwise_regularizer=l2_reg, pointwise_regularizer=l2_reg,\n",
    "            bias_regularizer=l2_reg, kernel_initializer='he_uniform'\n",
    "        )(x)\n",
    "\n",
    "        # Batch norm\n",
    "        x = keras.layers.BatchNormalization(axis=-1, momentum=0.99)(x)\n",
    "\n",
    "        # Activation\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "\n",
    "        # Dropout\n",
    "        if dropout is not None:\n",
    "            x = keras.layers.SpatialDropout2D(dropout)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def dense_op(x, n_units, bn, activation, l2_factor):\n",
    "\n",
    "        # Regularization\n",
    "        if l2_factor is not None:\n",
    "            l2_reg = keras.regularizers.l2(l2_factor)\n",
    "        else:\n",
    "            l2_reg = None\n",
    "\n",
    "        # Op\n",
    "        x = keras.layers.Dense(units=n_units, activation='linear', kernel_regularizer=l2_reg,\n",
    "                               bias_regularizer=l2_reg, kernel_initializer='he_uniform')(x)\n",
    "\n",
    "        # Batch norm\n",
    "        if bn:\n",
    "            x = keras.layers.BatchNormalization(axis=-1, momentum=0.99)(x)\n",
    "\n",
    "        # Activation\n",
    "        if activation == 'lrelu':\n",
    "            x = keras.layers.LeakyReLU()(x)\n",
    "        else:\n",
    "            x = keras.layers.Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Define classifier\n",
    "    input_x = keras.layers.Input(input_shape)\n",
    "    x = conv_op(input_x, stride=2)\n",
    "    x = conv_op(x, stride=2)\n",
    "    x = conv_op(x, stride=2)\n",
    "    x = conv_op(x, stride=2)\n",
    "    x = conv_op(x, stride=2)\n",
    "    x = conv_op(x, stride=2)\n",
    "    x = conv_op(x, stride=1)\n",
    "    x = conv_op(x, stride=1)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = dense_op(x, n_units=128, bn=True, activation='lrelu', l2_factor=1e-5)\n",
    "    x = dense_op(x, n_units=output_units, bn=False, activation='softmax', l2_factor=None)\n",
    "\n",
    "    # Compile\n",
    "    model = keras.models.Model(inputs=input_x, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=lr),\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        metrics=[keras.metrics.categorical_accuracy]\n",
    "    )\n",
    "\n",
    "    # print('Classifier model:', flush=True)\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(training_generator, validation_generator, output_dir, model, n_epochs, train_step_multiplier, workers,\n",
    "              patience, custom_objects=None, monitor='val_loss', mode='min', loss_list=['loss', 'val_loss'],\n",
    "              metric_list=['categorical_accuracy', 'val_categorical_accuracy'], val_step_multiplier=1.0, min_lr=1e-4,\n",
    "              extra_callbacks=[], cache_dir=None, lr_scheduler_fn=None):\n",
    "    # Cache output\n",
    "    if cache_dir is not None:\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "\n",
    "    # Ignore if training finished\n",
    "    if not os.path.exists(os.path.join(output_dir, 'training_finished.txt')):\n",
    "\n",
    "        # Prepare directory\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        print('Training model in directory: {d} with content {c}'.format(\n",
    "            d=output_dir,\n",
    "            c=os.system(\"ls \" + output_dir)\n",
    "        ), flush=True)\n",
    "\n",
    "        # Continue training if model found\n",
    "        epochs_run = 0\n",
    "        if os.path.exists(os.path.join(output_dir, 'last_epoch.h5')) and exists(join(output_dir, 'history.csv')):\n",
    "            print('Resuming training from saved model ...', flush=True)\n",
    "            model = keras.models.load_model(os.path.join(output_dir, 'last_epoch.h5'), custom_objects=custom_objects)\n",
    "            df = pd.read_csv(join(output_dir, 'history.csv'), header=0, index_col=0)\n",
    "            epochs_run = len(df)\n",
    "\n",
    "            # Copy existing files into cache\n",
    "            if cache_dir is not None and os.path.exists(cache_dir):\n",
    "                for path in glob(os.path.join(output_dir, '*')):\n",
    "                    try:\n",
    "                        shutil.copyfile(path, join(cache_dir, basename(path)))\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            'Error copying file {f} from external {output_dir} to cache {cache_dir} directory. Exception: {e}'.format(\n",
    "                                f=path, output_dir=output_dir, cache_dir=cache_dir, e=e\n",
    "                            ), flush=True)\n",
    "        else:\n",
    "            print('Training model from scratch {b1} {b2}...'.format(\n",
    "                b1=exists(join(output_dir, 'last_epoch.h5')),\n",
    "                b2=exists(join(output_dir, 'history.csv'))\n",
    "            ), flush=True)\n",
    "\n",
    "        if epochs_run < n_epochs:\n",
    "\n",
    "            if cache_dir is not None and exists(cache_dir):\n",
    "                external_output_dir = output_dir\n",
    "                output_dir = cache_dir\n",
    "            else:\n",
    "                external_output_dir = None\n",
    "\n",
    "            # Define callbacks\n",
    "            callback_list = [\n",
    "                StoreModelSummary(filepath=join(output_dir, 'model_summary.txt'), verbose=1),\n",
    "                HistoryCsv(file_path=join(output_dir, 'history.csv'))\n",
    "            ]\n",
    "\n",
    "            if len(extra_callbacks) > 0:\n",
    "                callback_list.extend(extra_callbacks)\n",
    "\n",
    "            callback_list2 = [\n",
    "                ModelCheckpoint(\n",
    "                    history_path=join(output_dir, 'history.csv'),\n",
    "                    filepath=join(output_dir, 'checkpoint.h5'),\n",
    "                    monitor=monitor,\n",
    "                    mode=mode,\n",
    "                    verbose=1,\n",
    "                    save_best_only=True\n",
    "                ),\n",
    "                ModelCheckpoint(\n",
    "                    history_path=join(output_dir, 'history.csv'),\n",
    "                    filepath=join(output_dir, 'last_epoch.h5'),\n",
    "                    monitor=monitor,\n",
    "                    mode=mode,\n",
    "                    verbose=1,\n",
    "                    save_best_only=False\n",
    "                ),\n",
    "                PlotHistory(\n",
    "                    plot_path=join(output_dir, 'history.png'),\n",
    "                    log_path=join(output_dir, 'history.csv'),\n",
    "                    loss_list=loss_list,\n",
    "                    metric_list=metric_list\n",
    "                ),\n",
    "                FinishedFlag(\n",
    "                    file_path=join(output_dir, 'training_finished.txt')\n",
    "                )\n",
    "            ]\n",
    "            callback_list.extend(callback_list2)\n",
    "\n",
    "            if patience is not None:\n",
    "                callback_list.append(\n",
    "                    ReduceLROnPlateau(\n",
    "                        history_path=join(output_dir, 'history.csv'),\n",
    "                        monitor=monitor,\n",
    "                        mode=mode,\n",
    "                        factor=1.0 / 3,\n",
    "                        patience=patience,\n",
    "                        verbose=1,\n",
    "                        cooldown=2,\n",
    "                        min_lr=min_lr\n",
    "                    ) if lr_scheduler_fn is None else LearningRateScheduler(schedule=lr_scheduler_fn, min_lr=min_lr)\n",
    "                )\n",
    "\n",
    "            if external_output_dir is not None:\n",
    "                callback_list.append(\n",
    "                    CopyResultsExternally(\n",
    "                        local_dir=output_dir,\n",
    "                        external_dir=external_output_dir\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Train model\n",
    "            model.fit_generator(\n",
    "                generator=training_generator,\n",
    "                steps_per_epoch=int(len(training_generator) * train_step_multiplier),\n",
    "                epochs=n_epochs,\n",
    "                verbose=1,\n",
    "                callbacks=callback_list,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=int(\n",
    "                    len(validation_generator) * val_step_multiplier) if validation_generator is not None else None,\n",
    "                initial_epoch=epochs_run,\n",
    "                max_queue_size=10,\n",
    "                workers=workers,\n",
    "                use_multiprocessing=True if workers > 1 else False\n",
    "            )\n",
    "\n",
    "            # Finish\n",
    "            try:\n",
    "                open(os.path.join(external_output_dir if external_output_dir is not None else output_dir,\n",
    "                                  'training_finished.txt'), 'a').close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "def run_train_model(cache_path, epochs, size_of_batch):\n",
    "    root_dir = '/home/user'\n",
    "    data_dir_luad = '/home/user/tcga_luad'\n",
    "    data_dir_lusc = '/home/user/tcga_lusc'\n",
    "    csv_path = '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/data/tcga/slide_list_tcga.csv'\n",
    "    output_dir = '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/model'\n",
    "\n",
    "    data_config = {'data_dir_luad': data_dir_luad, 'data_dir_lusc': data_dir_lusc, 'csv_path': csv_path}\n",
    "    image_ids_all, features_path, distance_map_path, labels_all, features_ids_all = read_data(data_config)\n",
    "\n",
    "\n",
    "\n",
    "    # Training set\n",
    "    crop_size = 400\n",
    "    code_size = 128\n",
    "    lr = 1e-2\n",
    "    output_units = 2\n",
    "    n_epochs = epochs\n",
    "    batch_size = size_of_batch\n",
    "    lr = 1e-2\n",
    "    code_size = 128\n",
    "    workers = 1\n",
    "    train_step_multiplier = 1\n",
    "    val_step_multiplier = 0.5\n",
    "    keep_data_training = 1\n",
    "    keep_data_validation = 1\n",
    "    patience = 4\n",
    "    occlusion_augmentation = None\n",
    "    elastic_augmentation = None\n",
    "    cache_dir = cache_path\n",
    "    occlusion_augmentation = False;\n",
    "    elastic_augmentation = False;\n",
    "    shuffle_augmentation = None;\n",
    "\n",
    "    print('Loading training set ...', flush=True)\n",
    "    training_gen = FeaturizedWsiGenerator(\n",
    "        data_config={'data_dir_luad': data_dir_luad, 'data_dir_lusc': data_dir_lusc, 'csv_path': csv_path},\n",
    "        data_fn=read_data,\n",
    "        batch_size=batch_size,\n",
    "        augment=True,\n",
    "        crop_size=400,\n",
    "        cache_dir=cache_dir,\n",
    "        balanced=True,\n",
    "        keep_data=keep_data_training,\n",
    "        occlusion_augmentation=occlusion_augmentation,\n",
    "        elastic_augmentation=elastic_augmentation,\n",
    "        shuffle_augmentation=shuffle_augmentation\n",
    "    )\n",
    "\n",
    "    # from nic.train_compressed_wsi import FeaturizedWsiSequence\n",
    "    keep_data_validation = 1\n",
    "    # Validation set\n",
    "    print('Loading validation set ...', flush=True)\n",
    "    use_validation = True\n",
    "    validation_gen = FeaturizedWsiSequence(\n",
    "        data_config={'data_dir_luad': data_dir_luad, 'data_dir_lusc': data_dir_lusc, 'csv_path': csv_path},\n",
    "        data_fn=read_data,\n",
    "        batch_size=batch_size,\n",
    "        crop_size=400,\n",
    "        cache_dir=cache_dir,\n",
    "        balanced=True,\n",
    "        keep_data=keep_data_validation\n",
    "    ) if use_validation else None\n",
    "\n",
    "    # Create model\n",
    "    print('Building model ...', flush=True)\n",
    "    model = None\n",
    "    if model is None:\n",
    "        model = build_wsi_classifier(input_shape=(crop_size, crop_size, code_size), lr=lr, output_units=output_units)\n",
    "\n",
    "    # Train initial model\n",
    "\n",
    "    loss_list = ['loss', 'val_loss']\n",
    "    metric_list = ['categorical_accuracy', 'val_categorical_accuracy']\n",
    "    custom_objects = None\n",
    "    lr_scheduler_fn = None\n",
    "    min_lr = 1e-4\n",
    "\n",
    "    print('Training model ...', flush=True)\n",
    "    fit_model(\n",
    "        training_generator=training_gen,\n",
    "        validation_generator=validation_gen,\n",
    "        output_dir=output_dir,\n",
    "        model=model,\n",
    "        n_epochs=n_epochs,\n",
    "        train_step_multiplier=train_step_multiplier,\n",
    "        val_step_multiplier=val_step_multiplier,\n",
    "        workers=workers,\n",
    "        patience=patience,\n",
    "        monitor='val_loss' if use_validation else 'loss',\n",
    "        mode='min',\n",
    "        loss_list=loss_list,\n",
    "        metric_list=metric_list,\n",
    "        custom_objects=custom_objects,\n",
    "        cache_dir=None if cache_dir is None else join(cache_dir, 'models', basename(output_dir)),\n",
    "        lr_scheduler_fn=lr_scheduler_fn,\n",
    "        min_lr=min_lr\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "from os.path import dirname\n",
    "\n",
    "\n",
    "def eval_model(model_path, data_dir, crop_size, output_path, cache_dir, batch_size,\n",
    "               custom_objects=None, keep_data=1.0):\n",
    "    # Output dir\n",
    "    if not exists(dirname(output_path)):\n",
    "        os.makedirs(dirname(output_path))\n",
    "\n",
    "    d = dirname(output_path)\n",
    "    print('Evaluating model in directory: {d} with content {c}'.format(\n",
    "        d=d,\n",
    "        c=os.system(\"ls \" + d)\n",
    "    ), flush=True)\n",
    "\n",
    "    # Test set\n",
    "    print('Loading test set ...', flush=True)\n",
    "    test_gen = FeaturizedWsiSequence(\n",
    "        data_config=data_dir,\n",
    "        data_fn=read_data,\n",
    "        batch_size=batch_size,\n",
    "        crop_size=crop_size,\n",
    "        cache_dir=cache_dir,\n",
    "        balanced=False,\n",
    "        keep_data=keep_data,\n",
    "        return_ids=True\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "    # Predictions\n",
    "    ids = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for i in range(len(test_gen)):\n",
    "\n",
    "        print('Predicting batch {i}/{n} ...'.format(i=i + 1, n=len(test_gen)), flush=True)\n",
    "        x, y, id = test_gen[i]\n",
    "\n",
    "        pred = model.predict_on_batch(x)\n",
    "        if pred.shape[-1] > 2:\n",
    "            pred = pred.argmax(axis=-1)\n",
    "        else:\n",
    "            pred = pred[:, 1]\n",
    "\n",
    "        ids.extend(id)\n",
    "        labels.extend(y.argmax(axis=-1))\n",
    "        preds.extend(pred)\n",
    "\n",
    "    # Format\n",
    "    df = pd.DataFrame({'id': ids, 'label': labels, 'pred': preds})\n",
    "    df = df.sort_values('id')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(output_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print('Failed to write file {f}. Exception: {e}'.format(f=output_path, e=e), flush=True)\n",
    "        d = dirname(output_path)\n",
    "        if not exists(d):\n",
    "            os.makedirs(d)\n",
    "        time.sleep(3)\n",
    "        df.to_csv(output_path)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def plot_roc(labels, preds, output_path=None, close_fig=True, legend_label=None):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # plt.figure()\n",
    "    lw = 2\n",
    "    l = 'ROC {tag}(area = {a:0.3f})'.format(tag='' if legend_label is None else legend_label, a=roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=lw, label=l)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.grid(b=True, which='both')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    if close_fig:\n",
    "        plt.close()\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "# Evaluate CNN\n",
    "def run_eval(data_config, output_dir, batch_size):\n",
    "    eval_model(\n",
    "        model_path=join(output_dir, 'last_epoch.h5'),\n",
    "        data_dir=data_config,\n",
    "        crop_size=400,\n",
    "        output_path=join(output_dir, 'eval', 'preds.csv'),\n",
    "        cache_dir=None,\n",
    "        batch_size=batch_size,\n",
    "        keep_data=1\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    from nic.train_compressed_wsi import compute_metrics\n",
    "    result_dir = output_dir\n",
    "    try:\n",
    "        compute_metrics(\n",
    "            input_path=join(result_dir, 'eval', 'preds.csv'),\n",
    "            output_dir=join(result_dir, 'eval')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to compute metrics. Exception: {e}'.format(e=e), flush=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Apply GradCAM analysis to CNN\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == '__main__':\n",
    "    #root_dir = r'E:/code/Project'\n",
    "    root_dir = '/home/user'\n",
    "    data_dir_luad = '/home/user/tcga_luad'\n",
    "    data_dir_lusc = '/home/user/tcga_lusc'\n",
    "    csv_path = '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/data/tcga/slide_list_tcga.csv'\n",
    "    output_dir = '/mnt/netcache/pathology/projects/pathology-lung-cancer-weak-growth-pattern-prediction/results/model'\n",
    "\n",
    "    cache_dir = None\n",
    "\n",
    "    data_config = {'data_dir_luad': data_dir_luad, 'data_dir_lusc': data_dir_lusc, 'csv_path': csv_path}\n",
    "    run_train_model(cache_dir, epochs=200, size_of_batch=8)\n",
    "    # run_eval(data_config, output_dir, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
