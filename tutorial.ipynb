{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Image Compression # \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join, dirname, basename, splitext, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import multiresolutionimageinterface as mri\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ##\n",
    "\n",
    "To demonstrate the functionality of NIC, we will need a set of whole-slide images (WSIs) with their respective slide-level labels. In this case, we will use the WSIs that can be found using the following pattern:\n",
    "\n",
    "`//chansey.umcn.nl/pathology/projects/pathology-liver-survival/data/images/Batch*/*.mrxs`\n",
    "\n",
    "And the slide-level labels can be found in column `HGP_SL` from:\n",
    "\n",
    "`//chansey.umcn.nl/pathology/projects/pathology-liver-survival/data/clinical/slide_list_hgpbin.csv`\n",
    "\n",
    "Additionally, column `partition` assigns each slide to a data fold (partitions 1 to 4 are used for cross-validation, and `encoder` to extract patches for encoder training -see next section-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'W:\\projects\\pathology-liver-survival'\n",
    "slide_dir = join(root_dir, 'data', 'images')\n",
    "csv_path = join(root_dir, 'data', 'clinical', 'slide_list_hgpbin.csv')\n",
    "cache_dir = None  # used to store local copies of files during I/O operations (useful in cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Encoder network ##\n",
    "\n",
    "To perform NIC, we will need an encoder network to transform small image patches into embedding vectors. According to the paper, BiGAN produces the best unsupervised encoder and it is the one we will train here.\n",
    "\n",
    "Alternatively, a collection of pretrained encoders (the one used in the NIC paper) can be found in \n",
    "\n",
    "`./models/encoders_patches_pathology/*.h5`\n",
    "\n",
    "Remember that these pretrained encoders accept 128x128x3 patches taken at 0.5 um/px resolution (often level 1), except for the BiGAN model that takes 64x64x3 at 1 um/px (often level 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the BiGAN model, we will first extract patches from the slides in the `encoder` partition. We will sample 10K patches per slide, producing ~260K patches in total. We select 96x96 patches to perform crop augmentation during training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.extract_patches import create_patch_dataset\n",
    "\n",
    "patches_npy_path = join(root_dir, 'results', 'patches', 'training.npy')\n",
    "\n",
    "# Extracts patches from whole-slide images and store them in a numpy array file\n",
    "create_patch_dataset(\n",
    "    input_dir=slide_dir,\n",
    "    csv_path=csv_path,\n",
    "    partition_tag='encoder',\n",
    "    output_path=patches_npy_path,\n",
    "    image_level=2,\n",
    "    patch_size=96,\n",
    "    n_patches_per_image=10000,\n",
    "    cache_dir=join(cache_dir, 'patches')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted the patches, we can proceed to train the BiGAN model. We will use the hyper-parameters described in the NIC paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.train_bigan_model import BiganModel\n",
    "\n",
    "model_bigan_dir = join(root_dir, 'results', 'encoders', 'bigan', 'rotterdam1_96_noaug', '0.0001')\n",
    "\n",
    "# Trains BiGAN\n",
    "bigan = BiganModel(\n",
    "    latent_dim=128,\n",
    "    n_filters=128,\n",
    "    lr=0.0001,\n",
    "    patch_size=64,\n",
    ")\n",
    "bigan.train(\n",
    "    x_path=patches_npy_path,\n",
    "    output_dir=model_bigan_dir,\n",
    "    epochs=400000,\n",
    "    batch_size=64,\n",
    "    sample_interval=1000,\n",
    "    save_models_on_epoch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware that training this model is highly unstable, thus it can fail or collapse with ease. If this happens, restart the training. Selecting a checkpoint model is a manual procedure: check the generated images and loss values and avoid abnormal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compress images ##\n",
    "\n",
    "Once we have a trained encoder, we can proceed with the WSI compression. I recommend running several `IDLE` instances of the following code in the cluster to speed up the lenghty process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the actual compression, we need to vectorize the WSIs. This process extracts all non-background patches from the slide and store them in numpy array format for quick access. In this case, we will read 64x64 patches at 1 um/px resolution (level 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.vectorize_wsi import vectorize_wsi\n",
    "\n",
    "def vectorize_images(input_dir, csv_path, output_dir, cache_dir, image_level, patch_size, overwrite):\n",
    "    \"\"\"\n",
    "    Converts a set of whole-slide images into numpy arrays with valid tissue patches for fast processing.\n",
    "\n",
    "    :param input_dir: folder containing the whole-slide images.\n",
    "    :param csv_path: list of slides.\n",
    "    :param output_dir: destination folder to store the vectorized images.\n",
    "    :param cache_dir: folder to store whole-slide images temporarily for fast access.\n",
    "    :param image_level: image resolution to read the patches.\n",
    "    :param patch_size: size of the read patches.\n",
    "    :param overwrite: True to overwrite existing images.\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # Output dir\n",
    "    if not exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read image file names\n",
    "    df = pd.read_csv(csv_path, header=0, index_col=0)\n",
    "\n",
    "    # Shuffle names\n",
    "    df = df.sample(len(df), replace=False)\n",
    "\n",
    "    # Process files\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "\n",
    "        try:\n",
    "            wsi_path = join(input_dir, row['batch'], row['slide_id'] + '.mrxs')\n",
    "            output_pattern = join(output_dir, row['slide_id'] + '_{item}.npy')\n",
    "            if overwrite or not exists(output_pattern.format(item='im_shape')):\n",
    "                print('Processing image {image}'.format(image=row['slide_id']), flush=True)\n",
    "                vectorize_wsi(\n",
    "                    image_path=cache_file(wsi_path, cache_dir, overwrite=False),\n",
    "                    mask_path=None,\n",
    "                    output_pattern=output_pattern,\n",
    "                    image_level=image_level,\n",
    "                    mask_level=None,\n",
    "                    patch_size=patch_size,\n",
    "                    stride=patch_size,\n",
    "                    downsample=1,\n",
    "                    select_bounding_box=True\n",
    "                )\n",
    "                print('Successful vectorized {image}'.format(image=row['slide_id']), flush=True)\n",
    "            else:\n",
    "                print('Already existing file {image}'.format(image=row['slide_id']), flush=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Failed to process image {row}. Exception: {e}'.format(row=row, e=e), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize WSIs\n",
    "\n",
    "vectorized_dir = join(root_dir, 'results', 'vectorized', 'rotterdam1')\n",
    "\n",
    "vectorize_images(\n",
    "    input_dir=slide_dir,\n",
    "    csv_path=csv_path,\n",
    "    output_dir=vectorized_dir,\n",
    "    cache_dir=join(cache_dir, 'vectorized'),\n",
    "    image_level=2,\n",
    "    patch_size=64,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compress the WSIs. Each WSI (vectorized file) will be processed 8 times due to WSI-level augmentation (rotation and flip). We will use an existing pretrained encoder from the NIC paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.featurize_wsi import encode_augment_wsi\n",
    "\n",
    "def featurize_images(input_dir, csv_path, model_path, output_dir, batch_size, overwrite):\n",
    "    \"\"\"\n",
    "    Compresses a set of whole-slide images using a trained encoder network.\n",
    "\n",
    "    :param input_dir: directory containing the compressed (featurized) images.\n",
    "    :param csv_path: path to list of slides.\n",
    "    :param model_path: path to trained encoder network.\n",
    "    :param output_dir: destination folder to store the compressed images.\n",
    "    :param batch_size: number of images to process in the GPU in one-go.\n",
    "    :param overwrite: True to overwrite existing files.\n",
    "    :return: nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Output dir\n",
    "    if not exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read image file names\n",
    "    df = pd.read_csv(csv_path, header=0, index_col=0)\n",
    "\n",
    "    # Shuffle names\n",
    "    df = df.sample(len(df), replace=False)\n",
    "\n",
    "    # Load encoder model\n",
    "    encoder = keras.models.load_model(\n",
    "        filepath=model_path\n",
    "    )\n",
    "\n",
    "    # Process files\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "\n",
    "        try:\n",
    "            wsi_pattern = join(input_dir, row['slide_id'] + '_{item}.npy')\n",
    "            if exists(wsi_pattern.format(item='im_shape')):\n",
    "                encode_augment_wsi(\n",
    "                    wsi_pattern=wsi_pattern,\n",
    "                    encoder=encoder,\n",
    "                    output_dir=output_dir,\n",
    "                    batch_size=batch_size,\n",
    "                    aug_modes=[('none', 0), ('none', 90), ('none', 180), ('none', 270), ('horizontal', 0), ('vertical', 0), ('vertical', 90), ('vertical', 270)],\n",
    "                    overwrite=overwrite\n",
    "                )\n",
    "            else:\n",
    "                print('Vectorized file not found: {f}'.format(f=wsi_pattern.format(item='im_shape')), flush=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Failed to process image {row}. Exception: {e}'.format(row=row, e=e), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize images\n",
    "\n",
    "featurized_dir = join(root_dir, 'results', 'featurized', 'rotterdam1', 'bigan', 'nic')\n",
    "model_path = join('models', 'encoders_patches_pathology', 'encoder_bigan.h5')\n",
    "\n",
    "featurize_images(\n",
    "    input_dir=vectorized_dir,\n",
    "    csv_path=csv_path,\n",
    "    output_dir=featurized_dir,\n",
    "    model_path=model_path,\n",
    "    batch_size=128,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train CNN on compressed images ##\n",
    "\n",
    "Once we have compressed the WSIs, we can proceed with the CNN classifier. In this example, we will train a classifier targeting the binary label `HGP_SL` found in the CSV file. We will be training 4 models using cross-validation: in each fold, we will use 2 data partitions for training, 1 for validation and 1 for testing. At the end of model training, we perform inference on the test set, compute metrics, and run GradCAM on the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.gradcam_wsi import gradcam_on_dataset\n",
    "from source.train_compressed_wsi import train_wsi_classifier, eval_model, compute_metrics\n",
    "\n",
    "def train_model(featurized_dir, csv_path, fold_n, output_dir, cache_dir, batch_size=16,\n",
    "                images_dir=None, vectorized_dir=None, lr=1e-2, patience=4,\n",
    "                occlusion_augmentation=False, elastic_augmentation=False, shuffle_augmentation=None):\n",
    "    \"\"\"\n",
    "    Trains a CNN using compressed whole-slide images.\n",
    "\n",
    "    :param featurized_dir: folder containing the compressed (featurized) images.\n",
    "    :param csv_path: list of slides with labels.\n",
    "    :param fold_n: fold determining which data partitions to use for training, validation and testing.\n",
    "    :param output_dir: destination folder to store results.\n",
    "    :param cache_dir: folder to store compressed images temporarily for fast access.\n",
    "    :param batch_size: number of samples to train with in one-go.\n",
    "    :return: nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Params\n",
    "    folds = [\n",
    "        {'training': ['partition_0', 'partition_1'], 'validation': ['partition_2'], 'test': ['partition_3']},\n",
    "        {'training': ['partition_1', 'partition_2'], 'validation': ['partition_3'], 'test': ['partition_0']},\n",
    "        {'training': ['partition_2', 'partition_3'], 'validation': ['partition_0'], 'test': ['partition_1']},\n",
    "        {'training': ['partition_3', 'partition_0'], 'validation': ['partition_1'], 'test': ['partition_2']},\n",
    "    ]\n",
    "    result_dir = join(output_dir, 'fold_{n}'.format(n=fold_n))\n",
    "\n",
    "    # Train CNN\n",
    "    train_wsi_classifier(\n",
    "        data_dir=featurized_dir,\n",
    "        csv_path=csv_path,\n",
    "        partitions=folds[fold_n],\n",
    "        crop_size=400,\n",
    "        output_dir=result_dir,\n",
    "        output_units=2,\n",
    "        cache_dir=cache_dir,\n",
    "        n_epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        code_size=128,\n",
    "        workers=1,\n",
    "        train_step_multiplier=1,\n",
    "        val_step_multiplier=0.5,\n",
    "        keep_data_training=1,\n",
    "        keep_data_validation=1,\n",
    "        patience=patience,\n",
    "        occlusion_augmentation=occlusion_augmentation,\n",
    "        elastic_augmentation=elastic_augmentation,\n",
    "        shuffle_augmentation=shuffle_augmentation\n",
    "    )\n",
    "\n",
    "    # Evaluate CNN\n",
    "    eval_model(\n",
    "        model_path=join(result_dir, 'checkpoint.h5'),\n",
    "        data_dir=featurized_dir,\n",
    "        csv_path=csv_path,\n",
    "        partitions=folds[fold_n],\n",
    "        crop_size=400,\n",
    "        output_path=join(result_dir, 'eval', 'preds.csv'),\n",
    "        cache_dir=cache_dir,\n",
    "        batch_size=batch_size,\n",
    "        keep_data=1\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    try:\n",
    "        compute_metrics(\n",
    "            input_path=join(result_dir, 'eval', 'preds.csv'),\n",
    "            output_dir=join(result_dir, 'eval')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to compute metrics. Exception: {e}'.format(e=e), flush=True)\n",
    "\n",
    "    # Apply GradCAM analysis to CNN\n",
    "    gradcam_on_dataset(\n",
    "        featurized_dir=featurized_dir,\n",
    "        csv_path=csv_path,\n",
    "        model_path=join(result_dir, 'checkpoint.h5'),\n",
    "        partitions=folds[fold_n]['test'],\n",
    "        layer_name='separable_conv2d_1',\n",
    "        output_unit=1,\n",
    "        custom_objects=None,\n",
    "        cache_dir=cache_dir,\n",
    "        images_dir=images_dir,\n",
    "        vectorized_dir=vectorized_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "\n",
    "selected_fold = 0\n",
    "model_dir = join(root_dir, 'results', 'models', 'rotterdam1', 'bigan', 'nic', 'hgp_bin')\n",
    "\n",
    "train_model(\n",
    "    featurized_dir=featurized_dir,\n",
    "    csv_path=csv_path,\n",
    "    fold_n=selected_fold, \n",
    "    output_dir=model_dir,\n",
    "    cache_dir=join(cache_dir, 'cnn'),\n",
    "    occlusion_augmentation=False,\n",
    "    lr=1e-2,\n",
    "    patience=4,\n",
    "    elastic_augmentation=False,\n",
    "    images_dir=slide_dir,  # required for GradCAM\n",
    "    vectorized_dir=vectorized_dir,  # required for GradCAM\n",
    "    shuffle_augmentation=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
